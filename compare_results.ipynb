{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and print results on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "from jiwer import wer, cer\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "def print_results(dfs: list[pd.DataFrame]):\n",
    "    def to_percent(num):\n",
    "        return round(num*100, 2)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.transcription = df.transcription.apply(lambda x: str(x).strip())\n",
    "        df[\"correct_prediction\"] = df.transcription == df.ground_truth\n",
    "        df[\"wer\"] = wer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "        df[\"cer\"] = cer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "\n",
    "        all_transcriptions = \" \".join(df.transcription)\n",
    "        all_gt = \" \".join(df.ground_truth)\n",
    "        all_wer = wer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        all_cer = cer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        mean_wer = df.wer.mean()\n",
    "        mean_cer = df.cer.mean()\n",
    "        accuracy = len(df[df.correct_prediction])/len(df)\n",
    "        \n",
    "        print(f\"\"\"{df.model_name[0]}\n",
    "    Strict accuracy:    {to_percent(accuracy)}%\n",
    "\n",
    "    Mean WER:           {to_percent(mean_wer)}%\n",
    "    WER (all concat):   {to_percent(all_wer)}%\n",
    "\n",
    "    Mean CER:           {to_percent(mean_cer)}%\n",
    "    CER (all concat):   {to_percent(all_cer)}%\\n\"\"\")\n",
    "        \n",
    "\n",
    "def dfs_to_score_df(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    scores = defaultdict(list)\n",
    "    for df in dfs:\n",
    "        df.transcription = df.transcription.apply(lambda x: str(x).strip())\n",
    "        df[\"correct_prediction\"] = df.transcription == df.ground_truth\n",
    "        df[\"wer\"] = wer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "        df[\"cer\"] = cer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "\n",
    "        all_transcriptions = \" \".join(df.transcription)\n",
    "        all_gt = \" \".join(df.ground_truth)\n",
    "        all_wer = wer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        all_cer = cer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        mean_wer = df.wer.mean()\n",
    "        mean_cer = df.cer.mean()\n",
    "        accuracy = len(df[df.correct_prediction])/len(df)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - Mean WER\")\n",
    "        scores[\"value\"].append(1-mean_wer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - Mean CER\")\n",
    "        scores[\"value\"].append(1-mean_cer)\n",
    "        \n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - WER (concat)\")\n",
    "        scores[\"value\"].append(1-all_wer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - CER (concat)\")\n",
    "        scores[\"value\"].append(1-all_cer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"Accuracy\")\n",
    "        scores[\"value\"].append(accuracy)\n",
    "    return pd.DataFrame(scores)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE THIS ###\n",
    "results_dirname = \"output/\"\n",
    "### ---------- ###\n",
    "\n",
    "p = Path(results_dirname)\n",
    "dfs = [pd.read_csv(e) for e in p.iterdir() if e.name.startswith(\"test\") and e.name.endswith(\"gt.csv\")]\n",
    "print_results(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = dfs_to_score_df(dfs)\n",
    "score_df.plot.bar(x='score', y='value', color='model', barmode=\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_sum = {\"model\": [], \"score_sum\": []}\n",
    "for model, df_ in score_df.groupby(\"model\"):\n",
    "    score_sum = df_.value.sum()\n",
    "    model_score_sum[\"model\"].append(model)\n",
    "    model_score_sum[\"score_sum\"].append(score_sum)\n",
    "\n",
    "pd.DataFrame(model_score_sum).plot.bar(x=\"model\", y=\"score_sum\", color=\"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_samisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
