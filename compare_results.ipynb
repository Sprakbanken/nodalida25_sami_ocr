{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize and print results on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "from jiwer import wer, cer\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "def print_results(dfs: list[pd.DataFrame]):\n",
    "    def to_percent(num):\n",
    "        return round(num*100, 2)\n",
    "\n",
    "    for df in dfs:\n",
    "        df.transcription = df.transcription.apply(lambda x: str(x).strip())\n",
    "        df[\"correct_prediction\"] = df.transcription == df.ground_truth\n",
    "        df[\"wer\"] = wer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "        df[\"cer\"] = cer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "\n",
    "        all_transcriptions = \" \".join(df.transcription)\n",
    "        all_gt = \" \".join(df.ground_truth)\n",
    "        all_wer = wer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        all_cer = cer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        mean_wer = df.wer.mean()\n",
    "        mean_cer = df.cer.mean()\n",
    "        accuracy = len(df[df.correct_prediction])/len(df)\n",
    "        \n",
    "        print(f\"\"\"{df.model_name[0]}\n",
    "    Strict accuracy:    {to_percent(accuracy)}%\n",
    "\n",
    "    Mean WER:           {to_percent(mean_wer)}%\n",
    "    WER (all concat):   {to_percent(all_wer)}%\n",
    "\n",
    "    Mean CER:           {to_percent(mean_cer)}%\n",
    "    CER (all concat):   {to_percent(all_cer)}%\\n\"\"\")\n",
    "        \n",
    "\n",
    "def dfs_to_score_df(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    scores = defaultdict(list)\n",
    "    for df in dfs:\n",
    "        df.transcription = df.transcription.apply(lambda x: str(x).strip())\n",
    "        df[\"correct_prediction\"] = df.transcription == df.ground_truth\n",
    "        df[\"wer\"] = wer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "        df[\"cer\"] = cer(reference=df.ground_truth.to_list(), hypothesis=df.transcription.to_list())\n",
    "\n",
    "        all_transcriptions = \" \".join(df.transcription)\n",
    "        all_gt = \" \".join(df.ground_truth)\n",
    "        all_wer = wer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        all_cer = cer(hypothesis=all_transcriptions, reference=all_gt)\n",
    "        mean_wer = df.wer.mean()\n",
    "        mean_cer = df.cer.mean()\n",
    "        accuracy = len(df[df.correct_prediction])/len(df)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - Mean WER\")\n",
    "        scores[\"value\"].append(1-mean_wer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - Mean CER\")\n",
    "        scores[\"value\"].append(1-mean_cer)\n",
    "        \n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - WER (concat)\")\n",
    "        scores[\"value\"].append(1-all_wer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"1 - CER (concat)\")\n",
    "        scores[\"value\"].append(1-all_cer)\n",
    "\n",
    "        scores[\"model\"].append(df.model_name[0])\n",
    "        scores[\"score\"].append(\"Accuracy\")\n",
    "        scores[\"value\"].append(accuracy)\n",
    "    return pd.DataFrame(scores)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE THIS ###\n",
    "results_dirname = \"output/\"\n",
    "### ---------- ###\n",
    "\n",
    "p = Path(results_dirname)\n",
    "ps = sorted([e for e in p.iterdir() if e.name.startswith(\"test\") and e.name.endswith(\"gt.csv\")])\n",
    "dfs = [pd.read_csv(e) for e in ps]\n",
    "print_results(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = dfs_to_score_df(dfs)\n",
    "score_df = score_df[score_df.score.apply(lambda x: \"Mean\" not in x)] # uncomment to include mean scores as well\n",
    "score_df = score_df[score_df.model.apply(lambda x: len(x.split(\"_\")) >= 3)] # uncomment to include giellatekno models\n",
    "# score_df = score_df[score_df.model.apply(lambda x: x.split(\"_\")[-1] in (\"25000\", \"40000\") or not x.split(\"_\")[-1].isnumeric())] # uncomment to include smaller models\n",
    "\n",
    "# score_df = score_df[score_df.model.apply(lambda x: \"transkribus\" not in x)] # uncomment to exclude transkribus models\n",
    "score_df.sort_values(\"model\").plot.bar(x='score', y='value', color='model', barmode=\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_sum = {\"model\": [], \"score_sum\": []}\n",
    "for model, df_ in score_df.groupby(\"model\"):\n",
    "    score_sum = df_.value.sum()\n",
    "    model_score_sum[\"model\"].append(model)\n",
    "    model_score_sum[\"score_sum\"].append(score_sum)\n",
    "\n",
    "pd.DataFrame(model_score_sum).sort_values(\"score_sum\").plot.bar(x=\"model\", y=\"score_sum\", color=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find test set examples where models perform well and not so well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [e.model_name[0] for e in dfs]\n",
    "dfs = [df for name, df in zip(names, dfs) if \"sme\" not in name]\n",
    "correct_predictions = pd.Series([0]*len(dfs[0]))\n",
    "\n",
    "for df in dfs:\n",
    "    correct_predictions += df.correct_prediction\n",
    "\n",
    "num_dfs = len(dfs)\n",
    "\n",
    "all_correct = correct_predictions.apply(lambda x: x == num_dfs)\n",
    "none_correct = correct_predictions.apply(lambda x: x == 0)\n",
    "\n",
    "all_correct_df = dfs[0][all_correct][[\"ground_truth\", \"image\"]]\n",
    "none_correct_df = dfs[0][none_correct][[\"ground_truth\", \"image\"]]\n",
    "len(all_correct_df), len(none_correct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    " \n",
    "testdata_path = Path(\"data/test\")\n",
    "\n",
    "streker = {'-', '–', '—', '_'}  \n",
    "contains_streker = 0\n",
    "\n",
    "for e in none_correct_df.itertuples():\n",
    "    chars = set(e.ground_truth)\n",
    "    if streker.intersection(chars):\n",
    "        contains_streker += 1\n",
    "    else:\n",
    "        img = Image.open(testdata_path / e.image)\n",
    "        print(\"\\n######################\")\n",
    "        print(e.image)\n",
    "        display(img)\n",
    "        print(e.ground_truth) \n",
    "\n",
    "len(none_correct_df), contains_streker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# testdata_path = Path(\"data/test\")\n",
    "\n",
    "# for e in all_correct_df.itertuples():\n",
    "#     img = Image.open(testdata_path / e.image)\n",
    "#     print(\"\\n######################\")\n",
    "#     display(img)\n",
    "#     print(e.ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# testdata_path = Path(\"data/test\")\n",
    "\n",
    "# for e in none_correct_df.itertuples():\n",
    "#     img = Image.open(testdata_path / e.image)\n",
    "#     print(\"\\n######################\")\n",
    "#     display(img)\n",
    "#     print(e.ground_truth)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_samisk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
